Building DAG of jobs...
Using shell: /usr/local/bin/bash
Provided cluster nodes: 1
Job counts:
	count	jobs
	1	all
	1	depth_of_cov
	2

rule depth_of_cov:
    input: /lustre/scratch118/malaria/team112/personal/tn6/pf_swga_cnv_wgs/input/ppq_resistance/PH1008-C.bam
    output: /lustre/scratch118/malaria/team112/personal/tn6/pf_swga_cnv_wgs/output/depth_of_cov/PH1008-C.unknown.unknown.sample_interval_statistics
    jobid: 1
    wildcards: library=unknown, sample=PH1008-C, lane=unknown

Submitted job 1 with external jobid 'Job <3014676> is submitted to queue <normal>.'.
    Error in rule depth_of_cov:
        jobid: 1
        output: /lustre/scratch118/malaria/team112/personal/tn6/pf_swga_cnv_wgs/output/depth_of_cov/PH1008-C.unknown.unknown.sample_interval_statistics

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /nfs/users/nfs_t/tn6/gitrepo/pf_swga_cnv/src/shell/.snakemake/log/2018-06-26T180714.320860.snakemake.log
