Building DAG of jobs...
Using shell: /usr/local/bin/bash
Provided cluster nodes: 1
Job counts:
	count	jobs
	1	all
	1	call_cnv
	1	filter_mean_rd
	1	filter_orig_rd
	1	find_sample_interval_summary_list
	1	interval_gc
	1	mean_rd
	1	merge_sample_interval_summary_list
	1	norm_by_pc
	1	output_cnv_csv
	1	pca_on_filtered_mean_rd
	1	zscore
	12

rule find_sample_interval_summary_list:
    output: /lustre/scratch118/malaria/team112/personal/tn6/pf_swga_cnv_wgs/output/xhmm/xhmm_test.sample_interval_summary.list.txt
    jobid: 11

Submitted job 11 with external jobid 'Job <3024375> is submitted to queue <normal>.'.
Finished job 11.
1 of 12 steps (8%) done

rule merge_sample_interval_summary_list:
    input: /lustre/scratch118/malaria/team112/personal/tn6/pf_swga_cnv_wgs/output/xhmm/xhmm_test.sample_interval_summary.list.txt
    output: /lustre/scratch118/malaria/team112/personal/tn6/pf_swga_cnv_wgs/output/xhmm/xhmm_test.RD.txt
    jobid: 8

Submitted job 8 with external jobid 'Job <3024376> is submitted to queue <normal>.'.
Terminating processes on user request, this might take some time.
Will exit after finishing currently running jobs.
Complete log: /nfs/users/nfs_t/tn6/gitrepo/pf_swga_cnv/src/shell/.snakemake/log/2018-06-27T060918.374031.snakemake.log
